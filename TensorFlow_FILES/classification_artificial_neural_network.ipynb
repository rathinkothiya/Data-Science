{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  benign_0__mal_1  \n",
       "0          0.4601                  0.11890                0  \n",
       "1          0.2750                  0.08902                0  \n",
       "2          0.3613                  0.08758                0  \n",
       "3          0.6638                  0.17300                0  \n",
       "4          0.2364                  0.07678                0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  benign_0__mal_1          569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_0__mal_1</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec096a0220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASZElEQVR4nO3df6zdd33f8ecrdhroYCKWbzJjO7WL3G4OA2fceW3ZpAxakiJtTliDnK3U3aKaPxKtaO2kBGlNaGcJNCjqOkA1IsT9MTKrQGNY19b1oIi2xFxnTohjvFh1SC727MuvETrJlZ33/rhff3Kwj+1jx997rn2fD+no+/1+vp/P97yPdOWXvz/O56SqkCQJ4KpxFyBJmj8MBUlSYyhIkhpDQZLUGAqSpGbxuAt4KZYuXVqrVq0adxmSdFnZs2fPN6pqYti+yzoUVq1axdTU1LjLkKTLSpKvnW2fl48kSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJzWX9jWbpSvbsr/79cZegeeiGX/lKr8fv7UwhycuS7E7yeJJ9Sd7TtT+Q5OtJ9navtw6MuS/JwSQHktzSV22SpOH6PFM4Drypqr6X5Grgi0n+R7fvg1X1/sHOSdYCG4EbgVcDf5rkR6rqZI81SpIG9HamULO+121e3b3O9YPQG4CHq+p4VR0CDgLr+6pPknSmXm80J1mUZC9wDNhZVY92u+5J8kSSB5Nc27UtB54bGD7dtZ1+zM1JppJMzczM9Fm+JC04vYZCVZ2sqnXACmB9ktcCHwFeA6wDjgAf6Lpn2CGGHHNrVU1W1eTExNDpwCVJF2lOHkmtqu8AnwduraqjXVi8AHyUFy8RTQMrB4atAA7PRX2SpFl9Pn00keRV3frLgZ8Evppk2UC324Enu/UdwMYk1yRZDawBdvdVnyTpTH0+fbQM2JZkEbPhs72qPpvkd5KsY/bS0DPAOwGqal+S7cBTwAngbp88kqS51VsoVNUTwE1D2t9xjjFbgC191SRJOjenuZAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqeguFJC9LsjvJ40n2JXlP174kyc4kT3fLawfG3JfkYJIDSW7pqzZJ0nB9nikcB95UVa8H1gG3Jvkx4F5gV1WtAXZ12yRZC2wEbgRuBT6cZFGP9UmSTtNbKNSs73WbV3evAjYA27r2bcBt3foG4OGqOl5Vh4CDwPq+6pMknanXewpJFiXZCxwDdlbVo8D1VXUEoFte13VfDjw3MHy6azv9mJuTTCWZmpmZ6bN8SVpweg2FqjpZVeuAFcD6JK89R/cMO8SQY26tqsmqmpyYmLhUpUqSmKOnj6rqO8Dnmb1XcDTJMoBueazrNg2sHBi2Ajg8F/VJkmb1+fTRRJJXdesvB34S+CqwA9jUddsEPNKt7wA2JrkmyWpgDbC7r/okSWda3OOxlwHbuieIrgK2V9Vnk/wlsD3JXcCzwB0AVbUvyXbgKeAEcHdVneyxPknSaXoLhap6ArhpSPs3gTefZcwWYEtfNUmSzs1vNEuSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1vYVCkpVJPpdkf5J9SX6xa38gydeT7O1ebx0Yc1+Sg0kOJLmlr9okScMt7vHYJ4BfqqrHkrwS2JNkZ7fvg1X1/sHOSdYCG4EbgVcDf5rkR6rqZI81SpIG9HamUFVHquqxbv15YD+w/BxDNgAPV9XxqjoEHATW91WfJOlMc3JPIckq4Cbg0a7pniRPJHkwybVd23LguYFh0wwJkSSbk0wlmZqZmemxaklaeHoPhSSvAD4JvKuqvgt8BHgNsA44AnzgVNchw+uMhqqtVTVZVZMTExM9VS1JC1OvoZDkamYD4feq6lMAVXW0qk5W1QvAR3nxEtE0sHJg+ArgcJ/1SZK+X59PHwX4GLC/qn59oH3ZQLfbgSe79R3AxiTXJFkNrAF291WfJOlMfT599EbgHcBXkuzt2t4N3JlkHbOXhp4B3glQVfuSbAeeYvbJpbt98kiS5lZvoVBVX2T4fYI/PMeYLcCWvmqSJJ2b32iWJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpKbPX167LLzh3//2uEvQPLTnP/3cuEuQxsIzBUlSYyhIkpqRQiHJrlHaJEmXt3OGQpKXJVkCLE1ybZIl3WsV8OrzjF2Z5HNJ9ifZl+QXu/YlSXYmebpbXjsw5r4kB5McSHLLS/94kqQLcb4zhXcCe4C/2y1PvR4BPnSesSeAX6qqvwf8GHB3krXAvcCuqloD7Oq26fZtBG4EbgU+nGTRxXwoSdLFOWcoVNVvVNVq4Jer6oeranX3en1V/ZfzjD1SVY91688D+4HlwAZgW9dtG3Bbt74BeLiqjlfVIeAgsP6iP5kk6YKN9EhqVf1mkp8AVg2OqaqRnufsLjfdBDwKXF9VR7rxR5Jc13VbDnxpYNh013b6sTYDmwFuuOGGUd5ekjSikUIhye8ArwH2Aie75gLOGwpJXgF8EnhXVX03yVm7DmmrMxqqtgJbASYnJ8/YL0m6eKN+eW0SWFtVF/SPcJKrmQ2E36uqT3XNR5Ms684SlgHHuvZpYOXA8BXA4Qt5P0nSSzPq9xSeBP7OhRw4s6cEHwP2V9WvD+zaAWzq1jcxe9P6VPvGJNckWQ2sAXZfyHtKkl6aUc8UlgJPJdkNHD/VWFX//Bxj3gi8A/hKkr1d27uB9wLbk9wFPAvc0R1rX5LtwFPMPrl0d1WdPPOwkqS+jBoKD1zogavqiwy/TwDw5rOM2QJsudD3kiRdGqM+ffRnfRciSRq/UZ8+ep4XnwT6AeBq4K+r6m/3VZgkae6NeqbwysHtJLfhF8sk6YpzUbOkVtUfAG+6xLVIksZs1MtHbxvYvIrZ7y34xTFJusKM+vTRPxtYPwE8w+xcRZKkK8io9xT+dd+FSJLGb9Qf2VmR5NNJjiU5muSTSVb0XZwkaW6NeqP548xOQ/FqZmcu/UzXJkm6gowaChNV9fGqOtG9HgImeqxLkjQGo4bCN5L8bJJF3etngW/2WZgkae6NGgr/Bng78H+AI8DPAN58lqQrzKiPpP4asKmqvg2QZAnwfmbDQpJ0hRj1TOF1pwIBoKq+xezPa0qSriCjhsJVSa49tdGdKYx6liFJukyM+g/7B4C/SPL7zE5v8Xb83QNJuuKM+o3m304yxewkeAHeVlVP9VqZJGnOjXwJqAsBg0CSrmAXNXW2JOnKZChIkpreQiHJg90Eek8OtD2Q5OtJ9navtw7suy/JwSQHktzSV12SpLPr80zhIeDWIe0frKp13esPAZKsBTYCN3ZjPpxkUY+1SZKG6C0UquoLwLdG7L4BeLiqjlfVIeAg/ga0JM25cdxTuCfJE93lpVNfiFsOPDfQZ7prO0OSzUmmkkzNzMz0XaskLShzHQofAV4DrGN2Yr0PdO0Z0nfob0BX1daqmqyqyYkJZ++WpEtpTkOhqo5W1cmqegH4KC9eIpoGVg50XQEcnsvaJElzHApJlg1s3g6cejJpB7AxyTVJVgNrgN1zWZskqcdJ7ZJ8ArgZWJpkGrgfuDnJOmYvDT0DvBOgqvYl2c7sN6ZPAHdX1cm+apMkDddbKFTVnUOaP3aO/ltwkj1JGiu/0SxJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLU9BYKSR5McizJkwNtS5LsTPJ0t7x2YN99SQ4mOZDklr7qkiSdXZ9nCg8Bt57Wdi+wq6rWALu6bZKsBTYCN3ZjPpxkUY+1SZKG6C0UquoLwLdOa94AbOvWtwG3DbQ/XFXHq+oQcBBY31dtkqTh5vqewvVVdQSgW17XtS8HnhvoN921nSHJ5iRTSaZmZmZ6LVaSFpr5cqM5Q9pqWMeq2lpVk1U1OTEx0XNZkrSwzHUoHE2yDKBbHuvap4GVA/1WAIfnuDZJWvDmOhR2AJu69U3AIwPtG5Nck2Q1sAbYPce1SdKCt7ivAyf5BHAzsDTJNHA/8F5ge5K7gGeBOwCqal+S7cBTwAng7qo62VdtkqTheguFqrrzLLvefJb+W4AtfdUjSTq/+XKjWZI0DxgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpWTyON03yDPA8cBI4UVWTSZYA/w1YBTwDvL2qvj2O+iRpoRrnmcI/rap1VTXZbd8L7KqqNcCubluSNIfm0+WjDcC2bn0bcNsYa5GkBWlcoVDAnyTZk2Rz13Z9VR0B6JbXDRuYZHOSqSRTMzMzc1SuJC0MY7mnALyxqg4nuQ7YmeSrow6sqq3AVoDJycnqq0BJWojGcqZQVYe75THg08B64GiSZQDd8tg4apOkhWzOQyHJ30ryylPrwFuAJ4EdwKau2ybgkbmuTZIWunFcProe+HSSU+//X6vqj5J8Gdie5C7gWeCOMdQmSQvanIdCVf0V8Poh7d8E3jzX9UiSXjSfHkmVJI2ZoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkpp5FwpJbk1yIMnBJPeOux5JWkjmVSgkWQR8CPhpYC1wZ5K1461KkhaOeRUKwHrgYFX9VVX9DfAwsGHMNUnSgrF43AWcZjnw3MD2NPCPBjsk2Qxs7ja/l+TAHNW2ECwFvjHuIuaDvH/TuEvQ9/Nv85T7cymO8kNn2zHfQmHYp63v26jaCmydm3IWliRTVTU57jqk0/m3OXfm2+WjaWDlwPYK4PCYapGkBWe+hcKXgTVJVif5AWAjsGPMNUnSgjGvLh9V1Ykk9wB/DCwCHqyqfWMuayHxspzmK/8250iq6vy9JEkLwny7fCRJGiNDQZLUGApyahHNW0keTHIsyZPjrmWhMBQWOKcW0Tz3EHDruItYSAwFObWI5q2q+gLwrXHXsZAYCho2tcjyMdUiacwMBZ13ahFJC4ehIKcWkdQYCnJqEUmNobDAVdUJ4NTUIvuB7U4tovkiySeAvwR+NMl0krvGXdOVzmkuJEmNZwqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKOiyk2TVpZhKOclkkv98KWoaOOaSJDuTPN0tr72Uxx/h/R9I8svn2H9Hkn1JXkgyOZe16fJgKGjBqqqpqvq3l/iw9wK7qmoNsKvbnk+eBN4GfGHchWh+MhR0uVqcZFuSJ5L8fpIfTPKGJH+WZE+SP06yDCDJ55O8L8nuJP87yT/p2m9O8tlufaL7n/1jSX4rydeSLO3OSvYn+Wj3P+w/SfLyc9S1AdjWrW8DbruQD5Xk55P8QZLPJDmU5J4k/y7J/0rypSRLun6/kOTLSR5P8skkPzjK8atqf1UduJCatLAYCrpc/SiwtapeB3wXuBv4TeBnquoNwIPAloH+i6tqPfAu4P4hx7sf+J9V9Q+ATwM3DOxbA3yoqm4EvgP8i3PUdX1VHQHoltddxGd7LfAvmf2tiy3A/6uqm5id7uHnuj6fqqp/WFWvZ3Z6Eqd/0CWxeNwFSBfpuar68279d4F3M/uP6c4kAIuAIwP9P9Ut9wCrhhzvHwO3A1TVHyX59sC+Q1W19zzjL6XPVdXzwPNJ/i/wma79K8DruvXXJvmPwKuAVzA7d5X0khkKulydPmnX88C+qvrxs/Q/3i1PMvzvftjvSpw+9tT4c10+OppkWVUd6S5fHTtH31He74WB7Rd4sfaHgNuq6vEkPw/cfBHvI53By0e6XN2Q5FQA3Al8CZg41Zbk6iQ3XsDxvgi8vRv7FuBinxraAWzq1jcBj1zkcc7nlcCRJFcD/6qn99ACZCjocrUf2JTkCWAJ3f0E4H1JHgf2Aj9xAcd7D/CWJI8BP83spafnL6Ku9wI/leRp4Ke67T78B+BRYCfw1VEHJbk9yTTw48B/T+JlJ30fp86WgCTXACer6kR3tvGRqlo37rqkueY9BWnWDcD2JFcBfwP8wpjrkcbCMwXpIiT5EPDG05p/o6o+PqTvLcD7Tmv+IeBrp7Udqqrb57o+aZChIElqvNEsSWoMBUlSYyhIkhpDQZLU/H+dJOtnTf90AAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='benign_0__mal_1',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('benign_0__mal_1',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using loss =binary cross entropy as we are solving the classification problem\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.7239 - val_loss: 0.6838\n",
      "Epoch 2/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6882 - val_loss: 0.6663\n",
      "Epoch 3/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6674 - val_loss: 0.6394\n",
      "Epoch 4/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6398 - val_loss: 0.6069\n",
      "Epoch 5/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6107 - val_loss: 0.5751\n",
      "Epoch 6/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5411\n",
      "Epoch 7/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5472 - val_loss: 0.5046\n",
      "Epoch 8/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5112 - val_loss: 0.4654\n",
      "Epoch 9/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4733 - val_loss: 0.4262\n",
      "Epoch 10/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4337 - val_loss: 0.3846\n",
      "Epoch 11/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.3514\n",
      "Epoch 12/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3183\n",
      "Epoch 13/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3298 - val_loss: 0.2857\n",
      "Epoch 14/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2960 - val_loss: 0.2565\n",
      "Epoch 15/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2700 - val_loss: 0.2361\n",
      "Epoch 16/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2485 - val_loss: 0.2175\n",
      "Epoch 17/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2338 - val_loss: 0.2020\n",
      "Epoch 18/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2191 - val_loss: 0.1875\n",
      "Epoch 19/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2053 - val_loss: 0.1757\n",
      "Epoch 20/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1954 - val_loss: 0.1730\n",
      "Epoch 21/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1869 - val_loss: 0.1589\n",
      "Epoch 22/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1779 - val_loss: 0.1570\n",
      "Epoch 23/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1707 - val_loss: 0.1434\n",
      "Epoch 24/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1628 - val_loss: 0.1444\n",
      "Epoch 25/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1575 - val_loss: 0.1402\n",
      "Epoch 26/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1519 - val_loss: 0.1298\n",
      "Epoch 27/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.1295\n",
      "Epoch 28/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1429 - val_loss: 0.1278\n",
      "Epoch 29/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1383 - val_loss: 0.1183\n",
      "Epoch 30/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1320 - val_loss: 0.1310\n",
      "Epoch 31/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.1131\n",
      "Epoch 32/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1205\n",
      "Epoch 33/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1103\n",
      "Epoch 34/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1218\n",
      "Epoch 35/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 0.1232\n",
      "Epoch 36/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1104 - val_loss: 0.1207\n",
      "Epoch 37/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.1179\n",
      "Epoch 38/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1060 - val_loss: 0.1198\n",
      "Epoch 39/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1015 - val_loss: 0.1202\n",
      "Epoch 40/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.1168\n",
      "Epoch 41/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1213\n",
      "Epoch 42/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.1177\n",
      "Epoch 43/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.1288\n",
      "Epoch 44/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0908 - val_loss: 0.1317\n",
      "Epoch 45/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0899 - val_loss: 0.1181\n",
      "Epoch 46/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0892 - val_loss: 0.1295\n",
      "Epoch 47/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0852 - val_loss: 0.1293\n",
      "Epoch 48/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0838 - val_loss: 0.1218\n",
      "Epoch 49/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0825 - val_loss: 0.1415\n",
      "Epoch 50/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0803 - val_loss: 0.1268\n",
      "Epoch 51/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.1472\n",
      "Epoch 52/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0782 - val_loss: 0.1284\n",
      "Epoch 53/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0772 - val_loss: 0.1451\n",
      "Epoch 54/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.1298\n",
      "Epoch 55/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0760 - val_loss: 0.1458\n",
      "Epoch 56/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0751 - val_loss: 0.1414\n",
      "Epoch 57/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0766 - val_loss: 0.1626\n",
      "Epoch 58/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.1294\n",
      "Epoch 59/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.1575\n",
      "Epoch 60/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0699 - val_loss: 0.1643\n",
      "Epoch 61/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0698 - val_loss: 0.1565\n",
      "Epoch 62/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.1611\n",
      "Epoch 63/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0682 - val_loss: 0.1728\n",
      "Epoch 64/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0667 - val_loss: 0.1618\n",
      "Epoch 65/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0661 - val_loss: 0.1777\n",
      "Epoch 66/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0657 - val_loss: 0.1686\n",
      "Epoch 67/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.1666\n",
      "Epoch 68/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0641 - val_loss: 0.1836\n",
      "Epoch 69/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0653 - val_loss: 0.1691\n",
      "Epoch 70/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0629 - val_loss: 0.1772\n",
      "Epoch 71/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0636 - val_loss: 0.1914\n",
      "Epoch 72/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.1748\n",
      "Epoch 73/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0608 - val_loss: 0.2137\n",
      "Epoch 74/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0629 - val_loss: 0.1788\n",
      "Epoch 75/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.1814\n",
      "Epoch 76/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0592 - val_loss: 0.2160\n",
      "Epoch 77/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0597 - val_loss: 0.2039\n",
      "Epoch 78/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.2206\n",
      "Epoch 79/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0593 - val_loss: 0.1846\n",
      "Epoch 80/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.2307\n",
      "Epoch 81/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0581 - val_loss: 0.1999\n",
      "Epoch 82/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.2366\n",
      "Epoch 83/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0564 - val_loss: 0.2073\n",
      "Epoch 84/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0584 - val_loss: 0.2197\n",
      "Epoch 85/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0603 - val_loss: 0.2353\n",
      "Epoch 86/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.2025\n",
      "Epoch 87/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.2563\n",
      "Epoch 88/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.2248\n",
      "Epoch 89/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.2558\n",
      "Epoch 90/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.2430\n",
      "Epoch 91/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.1942\n",
      "Epoch 92/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.2928\n",
      "Epoch 93/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.2421\n",
      "Epoch 94/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.2604\n",
      "Epoch 95/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.2563\n",
      "Epoch 96/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0526 - val_loss: 0.2669\n",
      "Epoch 97/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0517 - val_loss: 0.2385\n",
      "Epoch 98/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0512 - val_loss: 0.2553\n",
      "Epoch 99/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0508 - val_loss: 0.2601\n",
      "Epoch 100/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.2723\n",
      "Epoch 101/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.2713\n",
      "Epoch 102/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0499 - val_loss: 0.2702\n",
      "Epoch 103/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0502 - val_loss: 0.2792\n",
      "Epoch 104/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.2605\n",
      "Epoch 105/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.2673\n",
      "Epoch 106/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0528 - val_loss: 0.2684\n",
      "Epoch 107/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.2597\n",
      "Epoch 108/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.2765\n",
      "Epoch 109/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.2746\n",
      "Epoch 110/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.3319\n",
      "Epoch 111/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0501 - val_loss: 0.2549\n",
      "Epoch 112/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.3708\n",
      "Epoch 113/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.036 - 0s 2ms/step - loss: 0.0479 - val_loss: 0.2444\n",
      "Epoch 114/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0466 - val_loss: 0.3370\n",
      "Epoch 115/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.2751\n",
      "Epoch 116/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.3480\n",
      "Epoch 117/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.3151\n",
      "Epoch 118/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.3048\n",
      "Epoch 119/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.3399\n",
      "Epoch 120/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.3113\n",
      "Epoch 121/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.3655\n",
      "Epoch 122/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.3332\n",
      "Epoch 123/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.3263\n",
      "Epoch 124/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.3805\n",
      "Epoch 125/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.3333\n",
      "Epoch 126/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.3335\n",
      "Epoch 127/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.3458\n",
      "Epoch 128/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.3625\n",
      "Epoch 129/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.3820\n",
      "Epoch 130/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.3449\n",
      "Epoch 131/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.3704\n",
      "Epoch 132/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.3779\n",
      "Epoch 133/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.3733\n",
      "Epoch 134/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.3544\n",
      "Epoch 135/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.4220\n",
      "Epoch 136/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.3844\n",
      "Epoch 137/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0403 - val_loss: 0.3957\n",
      "Epoch 138/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.4236\n",
      "Epoch 139/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.3546\n",
      "Epoch 140/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.4138\n",
      "Epoch 141/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.4279\n",
      "Epoch 142/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.3920\n",
      "Epoch 143/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.4224\n",
      "Epoch 144/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.3846\n",
      "Epoch 145/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0380 - val_loss: 0.4499\n",
      "Epoch 146/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.4003\n",
      "Epoch 147/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.4391\n",
      "Epoch 148/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.4009\n",
      "Epoch 149/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.4069\n",
      "Epoch 150/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0431 - val_loss: 0.4466\n",
      "Epoch 151/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0377 - val_loss: 0.3979\n",
      "Epoch 152/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.4413\n",
      "Epoch 153/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.4080\n",
      "Epoch 154/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.4661\n",
      "Epoch 155/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0355 - val_loss: 0.4343\n",
      "Epoch 156/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.4610\n",
      "Epoch 157/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.4483\n",
      "Epoch 158/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.4934\n",
      "Epoch 159/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.4459\n",
      "Epoch 160/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.4882\n",
      "Epoch 161/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0345 - val_loss: 0.4473\n",
      "Epoch 162/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.5082\n",
      "Epoch 163/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0367 - val_loss: 0.4589\n",
      "Epoch 164/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.4407\n",
      "Epoch 165/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.5415\n",
      "Epoch 166/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.4675\n",
      "Epoch 167/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0350 - val_loss: 0.5388\n",
      "Epoch 168/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.4687\n",
      "Epoch 169/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0346 - val_loss: 0.5027\n",
      "Epoch 170/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.5191\n",
      "Epoch 171/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0328 - val_loss: 0.5230\n",
      "Epoch 172/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.5497\n",
      "Epoch 173/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.5420\n",
      "Epoch 174/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0332 - val_loss: 0.4503\n",
      "Epoch 175/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.5677\n",
      "Epoch 176/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.5566\n",
      "Epoch 177/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.4965\n",
      "Epoch 178/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.5575\n",
      "Epoch 179/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.5014\n",
      "Epoch 180/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.5626\n",
      "Epoch 181/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.5556\n",
      "Epoch 182/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0310 - val_loss: 0.5170\n",
      "Epoch 183/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0301 - val_loss: 0.5814\n",
      "Epoch 184/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.5317\n",
      "Epoch 185/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.6061\n",
      "Epoch 186/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0298 - val_loss: 0.5518\n",
      "Epoch 187/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.5928\n",
      "Epoch 188/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.6027\n",
      "Epoch 189/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 0.5721\n",
      "Epoch 190/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.5555\n",
      "Epoch 191/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.5946\n",
      "Epoch 192/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.5953\n",
      "Epoch 193/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0294 - val_loss: 0.5879\n",
      "Epoch 194/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.6317\n",
      "Epoch 195/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.5988\n",
      "Epoch 196/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.5781\n",
      "Epoch 197/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0296 - val_loss: 0.6131\n",
      "Epoch 198/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.6256\n",
      "Epoch 199/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.5836\n",
      "Epoch 200/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.6840\n",
      "Epoch 201/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0293 - val_loss: 0.5828\n",
      "Epoch 202/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.6935\n",
      "Epoch 203/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0339 - val_loss: 0.5847\n",
      "Epoch 204/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.6511\n",
      "Epoch 205/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.6608\n",
      "Epoch 206/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0277 - val_loss: 0.6261\n",
      "Epoch 207/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.6432\n",
      "Epoch 208/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.6886\n",
      "Epoch 209/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.6480\n",
      "Epoch 210/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.6985\n",
      "Epoch 211/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.5849\n",
      "Epoch 212/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.6737\n",
      "Epoch 213/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0259 - val_loss: 0.6390\n",
      "Epoch 214/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.7041\n",
      "Epoch 215/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0272 - val_loss: 0.6746\n",
      "Epoch 216/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.7170\n",
      "Epoch 217/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.6674\n",
      "Epoch 218/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.6183\n",
      "Epoch 219/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0238 - val_loss: 0.7315\n",
      "Epoch 220/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.6128\n",
      "Epoch 221/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.6966\n",
      "Epoch 222/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.7208\n",
      "Epoch 223/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0262 - val_loss: 0.5907\n",
      "Epoch 224/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.8035\n",
      "Epoch 225/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0232 - val_loss: 0.5701\n",
      "Epoch 226/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0265 - val_loss: 0.7591\n",
      "Epoch 227/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0281 - val_loss: 0.6929\n",
      "Epoch 228/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 0.7135\n",
      "Epoch 229/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0226 - val_loss: 0.6429\n",
      "Epoch 230/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.7290\n",
      "Epoch 231/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.6745\n",
      "Epoch 232/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.7764\n",
      "Epoch 233/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.7221\n",
      "Epoch 234/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.7533\n",
      "Epoch 235/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0229 - val_loss: 0.7770\n",
      "Epoch 236/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0221 - val_loss: 0.7309\n",
      "Epoch 237/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.7665\n",
      "Epoch 238/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.7411\n",
      "Epoch 239/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.7759\n",
      "Epoch 240/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.7607\n",
      "Epoch 241/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.7686\n",
      "Epoch 242/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.7759\n",
      "Epoch 243/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.7687\n",
      "Epoch 244/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.7505\n",
      "Epoch 245/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.7974\n",
      "Epoch 246/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0218 - val_loss: 0.7760\n",
      "Epoch 247/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.7423\n",
      "Epoch 248/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.8172\n",
      "Epoch 249/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.7421\n",
      "Epoch 250/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.7666\n",
      "Epoch 251/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.7692\n",
      "Epoch 252/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.7756\n",
      "Epoch 253/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.7418\n",
      "Epoch 254/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.8047\n",
      "Epoch 255/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.022 - 0s 2ms/step - loss: 0.0209 - val_loss: 0.8408\n",
      "Epoch 256/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0204 - val_loss: 0.8158\n",
      "Epoch 257/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0212 - val_loss: 0.7909\n",
      "Epoch 258/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.8449\n",
      "Epoch 259/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.8077\n",
      "Epoch 260/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.8816\n",
      "Epoch 261/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.7752\n",
      "Epoch 262/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.8289\n",
      "Epoch 263/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.8182\n",
      "Epoch 264/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.8812\n",
      "Epoch 265/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.8259\n",
      "Epoch 266/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0201 - val_loss: 0.8001\n",
      "Epoch 267/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.8577\n",
      "Epoch 268/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 0.8249\n",
      "Epoch 269/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.8762\n",
      "Epoch 270/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.8900\n",
      "Epoch 271/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.8048\n",
      "Epoch 272/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0198 - val_loss: 0.9219\n",
      "Epoch 273/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.8921\n",
      "Epoch 274/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.8240\n",
      "Epoch 275/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.008 - 0s 2ms/step - loss: 0.0186 - val_loss: 0.8302\n",
      "Epoch 276/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.8714\n",
      "Epoch 277/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.7722\n",
      "Epoch 278/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0188 - val_loss: 0.9278\n",
      "Epoch 279/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.8993\n",
      "Epoch 280/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0190 - val_loss: 0.9314\n",
      "Epoch 281/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.8707\n",
      "Epoch 282/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.9518\n",
      "Epoch 283/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.8509\n",
      "Epoch 284/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0192 - val_loss: 0.9155\n",
      "Epoch 285/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.8703\n",
      "Epoch 286/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.9096\n",
      "Epoch 287/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.9631\n",
      "Epoch 288/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.9118\n",
      "Epoch 289/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0186 - val_loss: 0.8519\n",
      "Epoch 290/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.9376\n",
      "Epoch 291/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.8651\n",
      "Epoch 292/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.9780\n",
      "Epoch 293/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.9025\n",
      "Epoch 294/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.9272\n",
      "Epoch 295/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.8750\n",
      "Epoch 296/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.9384\n",
      "Epoch 297/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.9776\n",
      "Epoch 298/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.8550\n",
      "Epoch 299/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0166 - val_loss: 0.9206\n",
      "Epoch 300/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.9605\n",
      "Epoch 301/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.9194\n",
      "Epoch 302/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.9583\n",
      "Epoch 303/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.8985\n",
      "Epoch 304/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.9693\n",
      "Epoch 305/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.9528\n",
      "Epoch 306/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0152 - val_loss: 0.9659\n",
      "Epoch 307/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.9918\n",
      "Epoch 308/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0158 - val_loss: 0.9696\n",
      "Epoch 309/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.9982\n",
      "Epoch 310/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.9320\n",
      "Epoch 311/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.9569\n",
      "Epoch 312/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 1.0274\n",
      "Epoch 313/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.9604\n",
      "Epoch 314/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 1.0693\n",
      "Epoch 315/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 0.9116\n",
      "Epoch 316/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.9673\n",
      "Epoch 317/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 1.0144\n",
      "Epoch 318/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 0.9371\n",
      "Epoch 319/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.9887\n",
      "Epoch 320/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 0.9574\n",
      "Epoch 321/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 1.0366\n",
      "Epoch 322/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 0.9195\n",
      "Epoch 323/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 1.0605\n",
      "Epoch 324/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.9971\n",
      "Epoch 325/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 1.0208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 326/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.9641\n",
      "Epoch 327/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 1.0569\n",
      "Epoch 328/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 1.0535\n",
      "Epoch 329/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 1.0240\n",
      "Epoch 330/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 1.0743\n",
      "Epoch 331/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0134 - val_loss: 0.9411\n",
      "Epoch 332/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 1.0927\n",
      "Epoch 333/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 1.0525\n",
      "Epoch 334/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 1.0526\n",
      "Epoch 335/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 1.0404\n",
      "Epoch 336/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 1.0642\n",
      "Epoch 337/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0151 - val_loss: 1.0338\n",
      "Epoch 338/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 1.0877\n",
      "Epoch 339/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 1.0422\n",
      "Epoch 340/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 1.0961\n",
      "Epoch 341/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0143 - val_loss: 1.1024\n",
      "Epoch 342/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 1.0519\n",
      "Epoch 343/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 1.0508\n",
      "Epoch 344/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 1.0658\n",
      "Epoch 345/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 1.0257\n",
      "Epoch 346/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 1.1176\n",
      "Epoch 347/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 1.0496\n",
      "Epoch 348/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 1.1564\n",
      "Epoch 349/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0159 - val_loss: 1.0189\n",
      "Epoch 350/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 1.1372\n",
      "Epoch 351/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 1.1080\n",
      "Epoch 352/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 1.0622\n",
      "Epoch 353/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 1.1083\n",
      "Epoch 354/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 1.0307\n",
      "Epoch 355/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 1.0903\n",
      "Epoch 356/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 1.1841\n",
      "Epoch 357/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 1.0261\n",
      "Epoch 358/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 1.1310\n",
      "Epoch 359/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 1.0897\n",
      "Epoch 360/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 1.1162\n",
      "Epoch 361/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0133 - val_loss: 1.1913\n",
      "Epoch 362/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 1.1122\n",
      "Epoch 363/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 1.2010\n",
      "Epoch 364/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 1.1118\n",
      "Epoch 365/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 1.2508\n",
      "Epoch 366/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0305 - val_loss: 1.1137\n",
      "Epoch 367/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0197 - val_loss: 1.2701\n",
      "Epoch 368/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 1.0632\n",
      "Epoch 369/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 1.1179\n",
      "Epoch 370/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 1.1437\n",
      "Epoch 371/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 1.0969\n",
      "Epoch 372/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 1.1247\n",
      "Epoch 373/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 1.0926\n",
      "Epoch 374/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 1.1337\n",
      "Epoch 375/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 1.1175\n",
      "Epoch 376/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 1.1846\n",
      "Epoch 377/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 1.1530\n",
      "Epoch 378/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 1.1228\n",
      "Epoch 379/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 1.1186\n",
      "Epoch 380/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 1.1846\n",
      "Epoch 381/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 1.2186\n",
      "Epoch 382/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0106 - val_loss: 1.1068\n",
      "Epoch 383/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 1.2196\n",
      "Epoch 384/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 1.1506\n",
      "Epoch 385/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 1.1841\n",
      "Epoch 386/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 1.1343\n",
      "Epoch 387/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 1.1844\n",
      "Epoch 388/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 1.1404\n",
      "Epoch 389/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.016 - 0s 2ms/step - loss: 0.0101 - val_loss: 1.1874\n",
      "Epoch 390/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 1.2310\n",
      "Epoch 391/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0107 - val_loss: 1.2203\n",
      "Epoch 392/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 1.2541\n",
      "Epoch 393/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0132 - val_loss: 1.2190\n",
      "Epoch 394/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 1.2421\n",
      "Epoch 395/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 1.1438\n",
      "Epoch 396/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 1.2097\n",
      "Epoch 397/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0137 - val_loss: 1.2800\n",
      "Epoch 398/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 1.2583\n",
      "Epoch 399/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 1.1536\n",
      "Epoch 400/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 1.1821\n",
      "Epoch 401/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0096 - val_loss: 1.2499\n",
      "Epoch 402/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 1.2624\n",
      "Epoch 403/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 1.2589\n",
      "Epoch 404/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 1.1939\n",
      "Epoch 405/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 1.3009\n",
      "Epoch 406/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 1.2299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 1.2788\n",
      "Epoch 408/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 1.2234\n",
      "Epoch 409/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 1.2729\n",
      "Epoch 410/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 1.2486\n",
      "Epoch 411/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 1.2411\n",
      "Epoch 412/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 1.2809\n",
      "Epoch 413/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0116 - val_loss: 1.2861\n",
      "Epoch 414/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 1.2824\n",
      "Epoch 415/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 1.2927\n",
      "Epoch 416/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 1.2801\n",
      "Epoch 417/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 1.1985\n",
      "Epoch 418/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 1.3400\n",
      "Epoch 419/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 1.1595\n",
      "Epoch 420/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 1.3048\n",
      "Epoch 421/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 1.3319\n",
      "Epoch 422/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 1.3051\n",
      "Epoch 423/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 1.3189\n",
      "Epoch 424/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 1.2176\n",
      "Epoch 425/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 1.2606\n",
      "Epoch 426/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 1.2860\n",
      "Epoch 427/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 1.3324\n",
      "Epoch 428/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 1.2344\n",
      "Epoch 429/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 1.3079\n",
      "Epoch 430/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 1.2955\n",
      "Epoch 431/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 1.3469\n",
      "Epoch 432/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 1.2236\n",
      "Epoch 433/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 1.3292\n",
      "Epoch 434/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 1.2431\n",
      "Epoch 435/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 1.3520\n",
      "Epoch 436/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 1.3513\n",
      "Epoch 437/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 1.3448\n",
      "Epoch 438/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 1.4022\n",
      "Epoch 439/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 1.2999\n",
      "Epoch 440/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 1.3094\n",
      "Epoch 441/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 1.3774\n",
      "Epoch 442/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 1.3885\n",
      "Epoch 443/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0100 - val_loss: 1.4165\n",
      "Epoch 444/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0091 - val_loss: 1.2619\n",
      "Epoch 445/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 1.3901\n",
      "Epoch 446/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 1.2356\n",
      "Epoch 447/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 1.4034\n",
      "Epoch 448/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 1.3302\n",
      "Epoch 449/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 1.3231\n",
      "Epoch 450/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 1.3754\n",
      "Epoch 451/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 1.3744\n",
      "Epoch 452/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0081 - val_loss: 1.2990\n",
      "Epoch 453/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 1.3541\n",
      "Epoch 454/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 1.3769\n",
      "Epoch 455/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 1.3411\n",
      "Epoch 456/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 1.4474\n",
      "Epoch 457/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 1.3408\n",
      "Epoch 458/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 1.4108\n",
      "Epoch 459/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 1.3912\n",
      "Epoch 460/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 1.4001\n",
      "Epoch 461/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 1.3565\n",
      "Epoch 462/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 1.4205\n",
      "Epoch 463/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 1.3290\n",
      "Epoch 464/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 1.4295\n",
      "Epoch 465/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 1.3617\n",
      "Epoch 466/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 1.5028\n",
      "Epoch 467/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 1.4624\n",
      "Epoch 468/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 1.4884\n",
      "Epoch 469/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 1.4234\n",
      "Epoch 470/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 1.4294\n",
      "Epoch 471/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 1.4051\n",
      "Epoch 472/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 1.4710\n",
      "Epoch 473/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 1.4319\n",
      "Epoch 474/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6740e-0 - 0s 2ms/step - loss: 0.0066 - val_loss: 1.4378\n",
      "Epoch 475/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 1.4804\n",
      "Epoch 476/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 1.4091\n",
      "Epoch 477/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 1.5436\n",
      "Epoch 478/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.3793\n",
      "Epoch 479/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 1.4182\n",
      "Epoch 480/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 1.4170\n",
      "Epoch 481/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 1.4910\n",
      "Epoch 482/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 1.5117\n",
      "Epoch 483/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 1.4442\n",
      "Epoch 484/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 1.5055\n",
      "Epoch 485/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 1.4422\n",
      "Epoch 486/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.5201\n",
      "Epoch 487/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 1.4858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 1.4503\n",
      "Epoch 489/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 1.5140\n",
      "Epoch 490/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 1.5097\n",
      "Epoch 491/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 1.4361\n",
      "Epoch 492/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.4649\n",
      "Epoch 493/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 1.5198\n",
      "Epoch 494/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 1.4457\n",
      "Epoch 495/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 1.6357\n",
      "Epoch 496/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 1.3786\n",
      "Epoch 497/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.6619\n",
      "Epoch 498/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 1.3799\n",
      "Epoch 499/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 1.6529\n",
      "Epoch 500/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 1.5754\n",
      "Epoch 501/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 1.5359\n",
      "Epoch 502/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 1.5467\n",
      "Epoch 503/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.5870\n",
      "Epoch 504/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 1.5537\n",
      "Epoch 505/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.5459\n",
      "Epoch 506/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.5499\n",
      "Epoch 507/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 1.5102\n",
      "Epoch 508/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.5669\n",
      "Epoch 509/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 1.5369\n",
      "Epoch 510/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.5516\n",
      "Epoch 511/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.6020\n",
      "Epoch 512/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 1.5084\n",
      "Epoch 513/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.6107\n",
      "Epoch 514/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.5464\n",
      "Epoch 515/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.6403\n",
      "Epoch 516/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 1.5273\n",
      "Epoch 517/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 1.6757\n",
      "Epoch 518/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 1.6057\n",
      "Epoch 519/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 1.6165\n",
      "Epoch 520/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 1.5921\n",
      "Epoch 521/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 1.6478\n",
      "Epoch 522/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 1.5789\n",
      "Epoch 523/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.5536\n",
      "Epoch 524/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6347\n",
      "Epoch 525/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.6057\n",
      "Epoch 526/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.6841\n",
      "Epoch 527/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0053 - val_loss: 1.6177\n",
      "Epoch 528/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.6540\n",
      "Epoch 529/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6279\n",
      "Epoch 530/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.6058\n",
      "Epoch 531/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.6279\n",
      "Epoch 532/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.7225\n",
      "Epoch 533/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.5700\n",
      "Epoch 534/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.7006\n",
      "Epoch 535/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 1.5691\n",
      "Epoch 536/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6645\n",
      "Epoch 537/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.6314\n",
      "Epoch 538/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 1.6232\n",
      "Epoch 539/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6915\n",
      "Epoch 540/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.6755\n",
      "Epoch 541/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 1.6457\n",
      "Epoch 542/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 1.6549\n",
      "Epoch 543/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6653\n",
      "Epoch 544/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 1.6687\n",
      "Epoch 545/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 1.6934\n",
      "Epoch 546/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6519\n",
      "Epoch 547/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6713\n",
      "Epoch 548/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.6436\n",
      "Epoch 549/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.7857\n",
      "Epoch 550/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 1.6068\n",
      "Epoch 551/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 1.7207\n",
      "Epoch 552/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.6473\n",
      "Epoch 553/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 1.7370\n",
      "Epoch 554/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 1.6997\n",
      "Epoch 555/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 1.7385\n",
      "Epoch 556/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 1.7143\n",
      "Epoch 557/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 1.7833\n",
      "Epoch 558/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 1.6832\n",
      "Epoch 559/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 1.7419\n",
      "Epoch 560/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.7543\n",
      "Epoch 561/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 1.6785\n",
      "Epoch 562/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.6518\n",
      "Epoch 563/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 1.7720\n",
      "Epoch 564/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.6868\n",
      "Epoch 565/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 1.8578\n",
      "Epoch 566/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 1.7407\n",
      "Epoch 567/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.8283\n",
      "Epoch 568/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.7315\n",
      "Epoch 569/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 1.8848\n",
      "Epoch 570/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 1.7003\n",
      "Epoch 571/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.8198\n",
      "Epoch 572/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.7683\n",
      "Epoch 573/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 1.7539\n",
      "Epoch 574/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.7407\n",
      "Epoch 575/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 1.6947\n",
      "Epoch 576/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.7952\n",
      "Epoch 577/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 1.7713\n",
      "Epoch 578/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 1.7450\n",
      "Epoch 579/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 1.8099\n",
      "Epoch 580/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 1.7678\n",
      "Epoch 581/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 1.7934\n",
      "Epoch 582/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 1.6603\n",
      "Epoch 583/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.9161\n",
      "Epoch 584/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 1.7779\n",
      "Epoch 585/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 1.7829\n",
      "Epoch 586/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 1.8495\n",
      "Epoch 587/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 1.7627\n",
      "Epoch 588/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 1.7861\n",
      "Epoch 589/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 1.7690\n",
      "Epoch 590/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 1.7614\n",
      "Epoch 591/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 1.9159\n",
      "Epoch 592/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.7121\n",
      "Epoch 593/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 1.8890\n",
      "Epoch 594/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 1.7624\n",
      "Epoch 595/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 1.9014\n",
      "Epoch 596/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 1.9372\n",
      "Epoch 597/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 1.8659\n",
      "Epoch 598/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 1.8319\n",
      "Epoch 599/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 1.8497\n",
      "Epoch 600/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 1.8360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec119d10d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.723876</td>\n",
       "      <td>0.683782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.688178</td>\n",
       "      <td>0.666329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.667434</td>\n",
       "      <td>0.639402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.639803</td>\n",
       "      <td>0.606871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.610740</td>\n",
       "      <td>0.575106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.004520</td>\n",
       "      <td>1.937155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.003660</td>\n",
       "      <td>1.865905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.003507</td>\n",
       "      <td>1.831860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.003223</td>\n",
       "      <td>1.849666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.003093</td>\n",
       "      <td>1.836040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  val_loss\n",
       "0    0.723876  0.683782\n",
       "1    0.688178  0.666329\n",
       "2    0.667434  0.639402\n",
       "3    0.639803  0.606871\n",
       "4    0.610740  0.575106\n",
       "..        ...       ...\n",
       "595  0.004520  1.937155\n",
       "596  0.003660  1.865905\n",
       "597  0.003507  1.831860\n",
       "598  0.003223  1.849666\n",
       "599  0.003093  1.836040\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec1a5b1a90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()\n",
    "#this is an example of overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using loss =binary cross entropy as we are solving the classification problem\n",
    "#using early stoping\n",
    "model=Sequential()\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Arguments:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the validation loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlstopping=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.6868 - val_loss: 0.6785\n",
      "Epoch 2/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6619 - val_loss: 0.6443\n",
      "Epoch 3/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6280 - val_loss: 0.5908\n",
      "Epoch 4/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5735 - val_loss: 0.5197\n",
      "Epoch 5/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5092 - val_loss: 0.4460\n",
      "Epoch 6/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4481 - val_loss: 0.3903\n",
      "Epoch 7/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3449\n",
      "Epoch 8/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3078\n",
      "Epoch 9/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3143 - val_loss: 0.2719\n",
      "Epoch 10/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2844 - val_loss: 0.2453\n",
      "Epoch 11/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 0.2241\n",
      "Epoch 12/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2379 - val_loss: 0.2063\n",
      "Epoch 13/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2211 - val_loss: 0.1921\n",
      "Epoch 14/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2061 - val_loss: 0.1787\n",
      "Epoch 15/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1945 - val_loss: 0.1680\n",
      "Epoch 16/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1832 - val_loss: 0.1587\n",
      "Epoch 17/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1760 - val_loss: 0.1486\n",
      "Epoch 18/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1673 - val_loss: 0.1408\n",
      "Epoch 19/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1598 - val_loss: 0.1328\n",
      "Epoch 20/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1511 - val_loss: 0.1273\n",
      "Epoch 21/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.1208\n",
      "Epoch 22/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1419 - val_loss: 0.1155\n",
      "Epoch 23/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.1110\n",
      "Epoch 24/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1296 - val_loss: 0.1063\n",
      "Epoch 25/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1231 - val_loss: 0.1048\n",
      "Epoch 26/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 0.0998\n",
      "Epoch 27/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1142 - val_loss: 0.1008\n",
      "Epoch 28/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1113 - val_loss: 0.0946\n",
      "Epoch 29/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.0921\n",
      "Epoch 30/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.0904\n",
      "Epoch 31/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.0960\n",
      "Epoch 32/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0998 - val_loss: 0.0903\n",
      "Epoch 33/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0887\n",
      "Epoch 34/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0945 - val_loss: 0.0905\n",
      "Epoch 35/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0925 - val_loss: 0.0881\n",
      "Epoch 36/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0887 - val_loss: 0.0857\n",
      "Epoch 37/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0869 - val_loss: 0.0941\n",
      "Epoch 38/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0868 - val_loss: 0.0906\n",
      "Epoch 39/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0904\n",
      "Epoch 40/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0813 - val_loss: 0.0916\n",
      "Epoch 41/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0801 - val_loss: 0.0951\n",
      "Epoch 42/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0910\n",
      "Epoch 43/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0811 - val_loss: 0.1036\n",
      "Epoch 44/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0761 - val_loss: 0.0913\n",
      "Epoch 45/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.1008\n",
      "Epoch 46/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0770 - val_loss: 0.0920\n",
      "Epoch 47/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0731 - val_loss: 0.1095\n",
      "Epoch 48/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0709 - val_loss: 0.0959\n",
      "Epoch 49/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0702 - val_loss: 0.1012\n",
      "Epoch 50/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.0987\n",
      "Epoch 51/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.1017\n",
      "Epoch 52/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0707 - val_loss: 0.1197\n",
      "Epoch 53/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.1053\n",
      "Epoch 54/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0667 - val_loss: 0.1086\n",
      "Epoch 55/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0648 - val_loss: 0.1014\n",
      "Epoch 56/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0639 - val_loss: 0.1110\n",
      "Epoch 57/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0659 - val_loss: 0.1161\n",
      "Epoch 58/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0629 - val_loss: 0.1117\n",
      "Epoch 59/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0622 - val_loss: 0.1127\n",
      "Epoch 60/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0615 - val_loss: 0.1226\n",
      "Epoch 61/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0608 - val_loss: 0.1077\n",
      "Epoch 00061: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec184977f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[earlstopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec1a131eb0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss=pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dropout layer to solve overfit\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using loss =binary cross entropy as we are solving the classification problem\n",
    "#using removing neurons\n",
    "model=Sequential()\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.7153 - val_loss: 0.6776\n",
      "Epoch 2/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6932 - val_loss: 0.6681\n",
      "Epoch 3/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6581\n",
      "Epoch 4/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6637 - val_loss: 0.6457\n",
      "Epoch 5/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6598 - val_loss: 0.6330\n",
      "Epoch 6/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6487 - val_loss: 0.6205\n",
      "Epoch 7/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6308 - val_loss: 0.6014\n",
      "Epoch 8/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6251 - val_loss: 0.5825\n",
      "Epoch 9/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.5583\n",
      "Epoch 10/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 0.5308\n",
      "Epoch 11/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5042\n",
      "Epoch 12/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5499 - val_loss: 0.4811\n",
      "Epoch 13/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.4456\n",
      "Epoch 14/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5047 - val_loss: 0.4106\n",
      "Epoch 15/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.3831\n",
      "Epoch 16/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 0.3541\n",
      "Epoch 17/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4397 - val_loss: 0.3309\n",
      "Epoch 18/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4064 - val_loss: 0.3069\n",
      "Epoch 19/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4201 - val_loss: 0.2894\n",
      "Epoch 20/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4273 - val_loss: 0.2757\n",
      "Epoch 21/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3889 - val_loss: 0.2659\n",
      "Epoch 22/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3892 - val_loss: 0.2510\n",
      "Epoch 23/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.2374\n",
      "Epoch 24/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.2283\n",
      "Epoch 25/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3487 - val_loss: 0.2171\n",
      "Epoch 26/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3379 - val_loss: 0.2021\n",
      "Epoch 27/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.1918\n",
      "Epoch 28/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3162 - val_loss: 0.1835\n",
      "Epoch 29/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 0.1727\n",
      "Epoch 30/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.3106 - val_loss: 0.1692\n",
      "Epoch 31/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2706 - val_loss: 0.1600\n",
      "Epoch 32/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2652 - val_loss: 0.1552\n",
      "Epoch 33/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2674 - val_loss: 0.1487\n",
      "Epoch 34/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.1461\n",
      "Epoch 35/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 0.1394\n",
      "Epoch 36/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2493 - val_loss: 0.1393\n",
      "Epoch 37/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2651 - val_loss: 0.1274\n",
      "Epoch 38/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2708 - val_loss: 0.1224\n",
      "Epoch 39/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2306 - val_loss: 0.1198\n",
      "Epoch 40/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2595 - val_loss: 0.1254\n",
      "Epoch 41/600\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.250 - 0s 2ms/step - loss: 0.2111 - val_loss: 0.1169\n",
      "Epoch 42/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2467 - val_loss: 0.1095\n",
      "Epoch 43/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2106 - val_loss: 0.1059\n",
      "Epoch 44/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2195 - val_loss: 0.1060\n",
      "Epoch 45/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2395 - val_loss: 0.1001\n",
      "Epoch 46/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2039 - val_loss: 0.0956\n",
      "Epoch 47/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2170 - val_loss: 0.0967\n",
      "Epoch 48/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1994 - val_loss: 0.0937\n",
      "Epoch 49/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2210 - val_loss: 0.0922\n",
      "Epoch 50/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1869 - val_loss: 0.0941\n",
      "Epoch 51/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2110 - val_loss: 0.0886\n",
      "Epoch 52/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.0877\n",
      "Epoch 53/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1973 - val_loss: 0.0830\n",
      "Epoch 54/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2135 - val_loss: 0.0816\n",
      "Epoch 55/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1696 - val_loss: 0.0830\n",
      "Epoch 56/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2072 - val_loss: 0.0845\n",
      "Epoch 57/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1974 - val_loss: 0.0774\n",
      "Epoch 58/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1785 - val_loss: 0.0772\n",
      "Epoch 59/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2082 - val_loss: 0.0842\n",
      "Epoch 60/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1941 - val_loss: 0.0796\n",
      "Epoch 61/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1754 - val_loss: 0.0739\n",
      "Epoch 62/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.2066 - val_loss: 0.0796\n",
      "Epoch 63/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1891 - val_loss: 0.0770\n",
      "Epoch 64/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.0731\n",
      "Epoch 65/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1908 - val_loss: 0.0837\n",
      "Epoch 66/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1672 - val_loss: 0.0823\n",
      "Epoch 67/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1545 - val_loss: 0.0829\n",
      "Epoch 68/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.0758\n",
      "Epoch 69/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1526 - val_loss: 0.0720\n",
      "Epoch 70/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1455 - val_loss: 0.0799\n",
      "Epoch 71/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1661 - val_loss: 0.0737\n",
      "Epoch 72/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1503 - val_loss: 0.0752\n",
      "Epoch 73/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.0681\n",
      "Epoch 74/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1655 - val_loss: 0.0803\n",
      "Epoch 75/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1518 - val_loss: 0.0791\n",
      "Epoch 76/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 0.0633\n",
      "Epoch 77/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1593 - val_loss: 0.0740\n",
      "Epoch 78/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1319 - val_loss: 0.0865\n",
      "Epoch 79/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.0728\n",
      "Epoch 80/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.0792\n",
      "Epoch 81/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1429 - val_loss: 0.0819\n",
      "Epoch 82/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1297 - val_loss: 0.0670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 0.0677\n",
      "Epoch 84/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1231 - val_loss: 0.0918\n",
      "Epoch 85/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1411 - val_loss: 0.0853\n",
      "Epoch 86/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1378 - val_loss: 0.0708\n",
      "Epoch 87/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.0705\n",
      "Epoch 88/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.0652\n",
      "Epoch 89/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.0804\n",
      "Epoch 90/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1310 - val_loss: 0.0784\n",
      "Epoch 91/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1050 - val_loss: 0.0796\n",
      "Epoch 92/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1258 - val_loss: 0.0745\n",
      "Epoch 93/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1293 - val_loss: 0.0799\n",
      "Epoch 94/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1415 - val_loss: 0.0814\n",
      "Epoch 95/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1212 - val_loss: 0.0821\n",
      "Epoch 96/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.0798\n",
      "Epoch 97/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1078 - val_loss: 0.0897\n",
      "Epoch 98/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1250 - val_loss: 0.1040\n",
      "Epoch 99/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1080 - val_loss: 0.1183\n",
      "Epoch 100/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1098 - val_loss: 0.0930\n",
      "Epoch 101/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.0966\n",
      "Epoch 00101: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec1a177730>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "         callbacks=[earlstopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec1b901ac0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss=pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95        67\n",
      "           1       0.99      0.95      0.97       121\n",
      "\n",
      "    accuracy                           0.96       188\n",
      "   macro avg       0.95      0.97      0.96       188\n",
      "weighted avg       0.96      0.96      0.96       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66   1]\n",
      " [  6 115]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
